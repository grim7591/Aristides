{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Aristides\n",
    "### A linear regression model for predicting the value of single family homes for tax purposes\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "### Data Pull\n",
    "The model is fed three years of qualified 0100 property sales from 2021 to 2023. Property factors are taken from the year after the sale. After one run of the model properties with a sale ratio that fell outside of 3 * IQR were then excluded as outliers per IAAO standards. Properties with a sale ratio that fell outside of 1.5 * IQR were reviewed and excluded or corrected if neccesary. \n",
    "\n",
    "### Property Factors\n",
    "The factors pulled from PACs to use for training and prediction are as follows:\n",
    "- legal_acreage\n",
    "- living_area\n",
    "- imprv_det_quality_cd\n",
    "- tax_area_description\n",
    "- abs_subdv_cd\n",
    "- sl_price\n",
    "- effective_year_built\n",
    "- imprv_type_cd\n",
    "- base_area\n",
    "- actual_year_built\n",
    "- prop_val_yr\n",
    "- total_porch_area (engineered in SQL)\n",
    "- total_garage_area (engineered in SQL)\n",
    "- effective_age (engineered in SQL)\n",
    "- has_canal (engineered in SQL)\n",
    "- has_lake (engineered in SQL)\n",
    "- number_of_baths (engineered in SQL)\n",
    "- MISC_Val\n",
    "\n",
    "### Market Areas\n",
    "Sean is most familiar with where these market areas came from. I believe we did some multivariate clustering in ArcGIS on the sale data. That got us market areas which were further subdivided into submarkets. This is the location component of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library and data import\n",
    "Importing python libraries, all of the data, market areas, removing properties with null values, and making sure everything is in a format the model can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from CSV files...\n",
      "Cleaning market area and sale data...\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from IAAOFunctions import PRD, COD, PRB, weightedMean, averageDeviation\n",
    "from StrataCaster import StrataCaster\n",
    "from PlotPlotter import PlotPlotter\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# Import Libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from IAAOFunctions import PRD, COD, PRB, weightedMean, averageDeviation\n",
    "from StrataCaster import StrataCaster\n",
    "from PlotPlotter import PlotPlotter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading data from CSV files...\")\n",
    "\n",
    "# Load data from multiple CSV files\n",
    "market_areas = pd.read_csv('Data/normalizedMAs.csv')\n",
    "sale_data = pd.read_csv(\"Data/dp51.csv\")\n",
    "\n",
    "Haile = pd.read_csv(\"Data/Haile.csv\")\n",
    "High_Springs_Main = pd.read_csv(\"Data/High_Springs_Main.csv\")\n",
    "Turkey_Creek = pd.read_csv(\"Data/Turkey_Creek.csv\")\n",
    "Alachua_Main = pd.read_csv(\"Data/Alachua_Main.csv\")\n",
    "Gainesvilleish_Region = pd.read_csv(\"Data/Gainesvilleish_Region.csv\")\n",
    "Real_Tioga = pd.read_csv(\"Data/Real_Tioga.csv\")\n",
    "Duck_Pond = pd.read_csv(\"Data/DuckPond.csv\")\n",
    "Newmans_Lake = pd.read_csv(\"Data/Newmans_Lake.csv\")\n",
    "EastMidtownEastA = pd.read_csv(\"Data/EastMidtownEastA.csv\")\n",
    "HighSpringsAGNV = pd.read_csv(\"Data/HighSpringsAGNV.csv\")\n",
    "Golfview = pd.read_csv(\"Data/Golfview.csv\")\n",
    "Lugano = pd.read_csv(\"Data/Lugano.csv\")\n",
    "Archer = pd.read_csv(\"Data/Archer.csv\")\n",
    "WildsPlantation = pd.read_csv(\"Data/WildsPlantation.csv\")\n",
    "Greystone = pd.read_csv(\"Data/Greystone.csv\")\n",
    "Eagle_Point = pd.read_csv(\"Data/Eagle_Point.csv\")\n",
    "Near_Haile = pd.read_csv(\"Data/Near_Haile.csv\")\n",
    "Buck_Bay = pd.read_csv(\"Data/Buck_Bay.csv\")\n",
    "Ironwood = pd.read_csv(\"Data/Ironwood.csv\")\n",
    "Serenola = pd.read_csv(\"Data/Serenola.csv\")\n",
    "BluesCreek = pd.read_csv(\"Data/BluesCreek.csv\")\n",
    "Edgemoore = pd.read_csv(\"Data/Edgemoore.csv\")\n",
    "SummerCreek = pd.read_csv(\"Data/SummerCreek.csv\")\n",
    "EastGNV = pd.read_csv(\"Data/EastGNV.csv\")\n",
    "TC_Forest = pd.read_csv(\"Data/TC_Forest.csv\")\n",
    "CarolEstates = pd.read_csv(\"Data/CarolEstates.csv\")\n",
    "Westchesterish = pd.read_csv(\"Data/Westchesterish.csv\")\n",
    "QuailCreekish = pd.read_csv(\"Data/QuailCreekish.csv\")\n",
    "\n",
    "# Clean the market area and sale data\n",
    "print(\"Cleaning market area and sale data...\")\n",
    "\n",
    "# Select only relevant columns from market_areas\n",
    "market_areas = market_areas[['prop_id', 'MA', 'Cluster ID', 'CENTROID_X', 'CENTROID_Y', 'geo_id']]\n",
    "\n",
    "# Remove rows with missing values\n",
    "market_areas.dropna(inplace=True)\n",
    "\n",
    "# Filter out rows with '<Null>' values\n",
    "market_areas = market_areas[market_areas['MA'] != '<Null>']\n",
    "market_areas = market_areas[market_areas['prop_id'] != '<Null>']\n",
    "\n",
    "# Convert 'prop_id' to string type\n",
    "market_areas['prop_id'] = market_areas['prop_id'].astype(str)\n",
    "\n",
    "# Convert 'prop_id' in sale_data to string type\n",
    "sale_data['prop_id'] = sale_data['prop_id'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Engineering\n",
    "The next step is to modify some of the factors we took out of PACs to make them more useful for model training. \n",
    "\n",
    " ### Creating Market_Cluster_ID\n",
    " Functionally there is only one location component and it's \"Market Cluster ID\". These clusters were cut out from the submarkets established in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Market Cluster ID...\n"
     ]
    }
   ],
   "source": [
    "# Factor engineer \"Market Cluster ID\"\n",
    "print(\"Creating Market Cluster ID...\")\n",
    "# Create a new column 'Market_Cluster_ID' by combining 'MA' and 'Cluster ID'\n",
    "market_areas['Market_Cluster_ID'] = market_areas['MA'].astype(str) + '_' + market_areas['Cluster ID'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overwriting the sale price of some properties whose sales were miscoded in PACs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_data.loc[sale_data['prop_id'] == '84296', 'sl_price'] = 90000\n",
    "sale_data.loc[sale_data['prop_id'] == '79157', 'sl_price'] = 300000\n",
    "sale_data.loc[sale_data['prop_id'] == '93683', 'sl_price'] = 199800\n",
    "sale_data.loc[sale_data['prop_id'] == '93443', 'sl_price'] = 132500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating \"Assessment_Val\"\n",
    "Assessment Value = 0.85 * (sale price - (MISC_Val/0.85)). This is the value the model will try to predict. Per statute we should aim to assess at 85% of purchase price to account for closing costs. MISC value is removed from the value used for training and testing because the model only acccounts for the lot and the base improvement, it has no way to meaningfully interpret and predict MISC value. I believe the MISC values that we have in PACs come from cost manuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor engineering Assessment Val...\n"
     ]
    }
   ],
   "source": [
    "# Factor engineer \"Assessment Val\"\n",
    "print(\"Factor engineering Assessment Val...\")\n",
    "# Calculate the 'Assessment_Val' based on the sale price and miscellaneous value\n",
    "sale_data['Assessment_Val'] = .85 * (sale_data['sl_price'] - (sale_data['MISC_Val'] / .85))\n",
    "# Add a validation step to ensure 'Assessment_Val' is not negative\n",
    "sale_data['Assessment_Val'] = sale_data['Assessment_Val'].apply(lambda x: x if x > 0 else np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating \"landiness\" \n",
    "landiness = legal_acreage / avg_legal_acreage. I also converted everything to square feet but I can't remember why.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating landiness...\n"
     ]
    }
   ],
   "source": [
    "# Factor engineer \"landiness\"\n",
    "print(\"Calculating landiness...\")\n",
    "# Calculate the average legal acreage in square feet\n",
    "avg_legal_acreage = (sale_data['legal_acreage'] * 43560).mean()\n",
    "# Create 'landiness' as a ratio of property acreage to average acreage\n",
    "sale_data['landiness'] = (sale_data['legal_acreage'] * 43560) / avg_legal_acreage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the pulled sales data with the market area spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging market area and sale data...\n"
     ]
    }
   ],
   "source": [
    "# Merge the market area and sale data\n",
    "print(\"Merging market area and sale data...\")\n",
    "# Merge sale_data and market_areas on 'prop_id'\n",
    "result = pd.merge(sale_data, market_areas, how='inner', on='prop_id')\n",
    "# Drop rows with missing values after merging\n",
    "result.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating in_subdivision\n",
    "Binary variable for if a property is in a subdivision or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating binary variables for subdivision status...\n"
     ]
    }
   ],
   "source": [
    "# Make subdivision code binary variable\n",
    "print(\"Creating binary variables for subdivision status...\")\n",
    "# Create a binary variable 'in_subdivision' to indicate if property is in a subdivision\n",
    "result['in_subdivision'] = result['abs_subdv_cd'].apply(lambda x: True if x > 0 else False)\n",
    "# Drop unnecessary columns\n",
    "result = result.drop(columns=['abs_subdv_cd', 'MA', 'Cluster ID'])\n",
    "\n",
    "# Convert 'prop_id' to string for consistency across dataframes\n",
    "result['prop_id'] = result['prop_id'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effective age overwrites\n",
    "In 2024 we updated the effective year built of all properties to 1994 at minimum. When reviewing outliers I applied that same logic to these properties which were evaluated on pre-2024 factors. Now that I think about it I might want to do some kind of \"if prop_val_yr - 10 is > eff_yr_blt then eff_yr_blt = prop_val_yr - 10\" type thing. It was determined by valution that a 10 year limit on effective age makes sense and because that was a change in our process and not neccesarily a market shift, I think it makes sense to mitigate the impact of that change on the model by applying it retroactively to previous sale years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effage overwrites\n",
    "result.loc[result['prop_id'].isin(['85636']), 'effective_age'] = (result['prop_val_yr'] - 1) - 1992\n",
    "result.loc[result['prop_id'].isin(['98109']), 'effective_age'] = (result['prop_val_yr'] - 1) - 1993\n",
    "result.loc[result['prop_id'].isin(['47151']), 'effective_age'] = (result['prop_val_yr'] - 1) - 1992\n",
    "result.loc[result['prop_id'].isin(['92312']), 'effective_age'] = (result['prop_val_yr'] - 1) - 1993\n",
    "result.loc[result['prop_id'].isin(['14875']), 'effective_age'] = (result['prop_val_yr'] - 1) - 1992\n",
    "result.loc[result['prop_id'].isin(['92322']), 'effective_age'] = (result['prop_val_yr'] - 1) - 1992\n",
    "result.loc[result['prop_id'].isin(['87817']), 'effective_age'] = (result['prop_val_yr'] - 1) - 1993\n",
    "result.loc[result['prop_id'].isin(['9073']), 'effective_age'] = (result['prop_val_yr'] - 1) - 1993\n",
    "result.loc[result['prop_id'].isin(['66141']), 'effective_age'] = (result['prop_val_yr'] - 1) - 1993\n",
    "result.loc[result['prop_id'].isin(['86173']), 'effective_age'] = (result['prop_val_yr'] - 1) - 1993\n",
    "result.loc[result['prop_id'].isin(['81469']), 'effective_age'] = (result['prop_val_yr'] - 1) - 1993\n",
    "result.loc[result['prop_id'].isin(['95004']), 'effective_age'] = (result['prop_val_yr'] - 1) - 1993"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating \"percent good\" from effective age\n",
    "Percent good = 1 - (effective_age/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating percent good based on effective age...\n"
     ]
    }
   ],
   "source": [
    "# Factor Engineer Percent Good based on effective age\n",
    "print(\"Calculating percent good based on effective age...\")\n",
    "# Calculate 'percent_good' as a factor of effective age\n",
    "result['percent_good'] = 1 - (result['effective_age']/ 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality code overwrites\n",
    "When reviewing outliers I found some properties that needed to have their quality codes overwritten. As far as I understand it, because these are prior year factors, we are not able to edit them in PACs so I edit them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.loc[result['prop_id'].isin(['96615']), 'imprv_det_quality_cd'] = 1\n",
    "\n",
    "result.loc[result['prop_id'].isin(['96411']), 'imprv_det_quality_cd'] = 2\n",
    "\n",
    "result.loc[result['prop_id'].isin(['13894']), 'imprv_det_quality_cd'] = 2\n",
    "\n",
    "result.loc[result['prop_id'].isin(['8894']), 'imprv_det_quality_cd'] = 2\n",
    "\n",
    "result.loc[result['prop_id'].isin(['19165']), 'imprv_det_quality_cd'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearize the quality codes\n",
    "Still working with Michael on this. For now I'm using what is already in PACs but here are the two linearizations we've been testing.\n",
    "\n",
    "    1: 0.75,\n",
    "    2: 0.90,\n",
    "    3: 1.00,\n",
    "    4: 1.15,\n",
    "    5: 1.40,\n",
    "    6: 1.70\n",
    "\n",
    "    1: 0.1331291,\n",
    "    2: 0.5665645,\n",
    "    3: 1.0,\n",
    "    4: 1.1624432,\n",
    "    5: 1.4343298,\n",
    "    6: 1.7062164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linearizing quality codes...\n"
     ]
    }
   ],
   "source": [
    "# Linearize the quality codes\n",
    "print(\"Linearizing quality codes...\")\n",
    "# Replace quality codes with numerical values for linear regression\n",
    "result['imprv_det_quality_cd'] = result['imprv_det_quality_cd'].replace({\n",
    "    1: 0.75,\n",
    "    2: 0.90,\n",
    "    3: 1.00,\n",
    "    4: 1.15,\n",
    "    5: 1.40,\n",
    "    6: 1.70\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding handcrafted Market Cluster ID's\n",
    "Using the results of the model as guide we futher subdivided the properties in the initial market clusters based on comparable valuation, geography, tax areas, well defined and/or unique neighborhoods, etc. These are still in progress at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating Market Cluster IDs for new subdivisions...\n"
     ]
    }
   ],
   "source": [
    "# New Market Area subdivisions\n",
    "print(\"Updating Market Cluster IDs for new subdivisions...\")\n",
    "\n",
    "# Ensure 'prop_id' is a string for all subdivision dataframes\n",
    "Haile['prop_id'] = Haile['prop_id'].astype(str)\n",
    "High_Springs_Main['prop_id'] = High_Springs_Main['prop_id'].astype(str)\n",
    "Turkey_Creek['prop_id'] = Turkey_Creek['prop_id'].astype(str)\n",
    "Alachua_Main['prop_id'] = Alachua_Main['prop_id'].astype(str)\n",
    "Gainesvilleish_Region['prop_id'] = Gainesvilleish_Region['prop_id'].astype(str)\n",
    "Real_Tioga['prop_id'] = Real_Tioga['prop_id'].astype(str)\n",
    "Duck_Pond['prop_id'] = Duck_Pond['prop_id'].astype(str)\n",
    "Newmans_Lake['prop_id'] = Newmans_Lake['prop_id'].astype(str)\n",
    "EastMidtownEastA['prop_id'] = EastMidtownEastA['prop_id'].astype(str)\n",
    "HighSpringsAGNV['prop_id'] = HighSpringsAGNV['prop_id'].astype(str)\n",
    "Golfview['prop_id'] = Golfview['prop_id'].astype(str)\n",
    "Lugano['prop_id'] = Lugano['prop_id'].astype(str)\n",
    "Archer['prop_id'] = Archer['prop_id'].astype(str)\n",
    "WildsPlantation['prop_id'] = WildsPlantation['prop_id'].astype(str)\n",
    "Greystone['prop_id'] = Greystone['prop_id'].astype(str)\n",
    "Eagle_Point['prop_id'] = Eagle_Point['prop_id'].astype(str)\n",
    "Near_Haile['prop_id'] = Near_Haile['prop_id'].astype(str)\n",
    "Buck_Bay['prop_id'] = Buck_Bay['prop_id'].astype(str)\n",
    "EastGNV['prop_id'] = EastGNV['prop_id'].astype(str)\n",
    "SummerCreek['prop_id'] = SummerCreek['prop_id'].astype(str)\n",
    "Ironwood['prop_id'] = Ironwood['prop_id'].astype(str)\n",
    "TC_Forest['prop_id'] = TC_Forest['prop_id'].astype(str)\n",
    "CarolEstates['prop_id'] = CarolEstates['prop_id'].astype(str)\n",
    "Westchesterish['prop_id'] = Westchesterish['prop_id'].astype(str)\n",
    "QuailCreekish['prop_id'] = QuailCreekish['prop_id'].astype(str)\n",
    "\n",
    "# Assign new Market Cluster IDs based on subdivision membership and tax area description\n",
    "result.loc[result['prop_id'].isin(Haile['prop_id']), 'Market_Cluster_ID'] = 'HaileLike'\n",
    "result.loc[result['tax_area_description'] == 'LACROSSE', 'Market_Cluster_ID'] = 'HSBUI'\n",
    "result.loc[result['tax_area_description'] == 'HAWTHORNE', 'Market_Cluster_ID'] = 'Hawthorne'\n",
    "result.loc[result['Market_Cluster_ID'] == 'HighSprings_D', 'Market_Cluster_ID'] = 'High_Springs_Main'\n",
    "result.loc[result['Market_Cluster_ID'] == 'MidtownEast_E', 'Market_Cluster_ID'] = 'MidtownEast_C'\n",
    "result.loc[result['Market_Cluster_ID'] == 'MidtownEast_F', 'Market_Cluster_ID'] = 'MidtownEast_B'\n",
    "result.loc[result['Market_Cluster_ID'] == 'HighSprings_C', 'Market_Cluster_ID'] = 'HSBUI'\n",
    "result.loc[result['Market_Cluster_ID'] == 'Springtree_C', 'Market_Cluster_ID'] = 'HSBUI'\n",
    "result.loc[result['Market_Cluster_ID'] == 'swNewberry_C', 'Market_Cluster_ID'] = 'HSBUI'\n",
    "result.loc[result['prop_id'].isin(High_Springs_Main['prop_id']), 'Market_Cluster_ID'] = 'High_Springs_Main'\n",
    "result.loc[result['prop_id'].isin(Turkey_Creek['prop_id']), 'Market_Cluster_ID'] = 'Turkey_Creek'\n",
    "result.loc[result['prop_id'].isin(Alachua_Main['prop_id']), 'Market_Cluster_ID'] = 'Alachua_Main'\n",
    "result.loc[result['prop_id'].isin(Gainesvilleish_Region['prop_id']), 'Market_Cluster_ID'] = 'Gainesvilleish_Region'\n",
    "result.loc[result['prop_id'].isin(Real_Tioga['prop_id']), 'Market_Cluster_ID'] = 'Real_Tioga'\n",
    "result.loc[result['prop_id'].isin(Duck_Pond['prop_id']), 'Market_Cluster_ID'] = 'Duck_Pond'\n",
    "result.loc[result['prop_id'].isin(Newmans_Lake['prop_id']), 'Market_Cluster_ID'] = 'Newmans_Lake'\n",
    "result.loc[result['prop_id'].isin(EastMidtownEastA['prop_id']), 'Market_Cluster_ID'] = 'EastMidtownEastA'\n",
    "result.loc[result['prop_id'].isin(HighSpringsAGNV['prop_id']), 'Market_Cluster_ID'] = 'HighSpringsAGNV'\n",
    "result.loc[result['prop_id'].isin(Golfview['prop_id']), 'Market_Cluster_ID'] = 'Golfview'\n",
    "result.loc[result['prop_id'].isin(Lugano['prop_id']), 'Market_Cluster_ID'] = 'Lugano'\n",
    "result.loc[result['prop_id'].isin(Archer['prop_id']), 'Market_Cluster_ID'] = 'Archer'\n",
    "result.loc[result['prop_id'].isin(WildsPlantation['prop_id']), 'Market_Cluster_ID'] = 'WildsPlantation'\n",
    "result.loc[result['prop_id'].isin(Greystone['prop_id']), 'Market_Cluster_ID'] = 'HaileLike'\n",
    "result.loc[result['prop_id'].isin(Near_Haile['prop_id']), 'Market_Cluster_ID'] = 'HaileLike'\n",
    "result.loc[result['prop_id'].isin(Buck_Bay['prop_id']), 'Market_Cluster_ID'] = 'Buck_Bay'\n",
    "result.loc[result['prop_id'].isin(EastGNV['prop_id']), 'Market_Cluster_ID'] = 'EastGNV'\n",
    "result.loc[result['prop_id'].isin(SummerCreek['prop_id']), 'Market_Cluster_ID'] = 'SummerCreek'\n",
    "result.loc[result['prop_id'].isin(Ironwood['prop_id']), 'Market_Cluster_ID'] = 'Ironwood'\n",
    "result.loc[result['prop_id'].isin(TC_Forest['prop_id']), 'Market_Cluster_ID'] = 'TC_Forest'\n",
    "result.loc[result['prop_id'].isin(CarolEstates['prop_id']), 'Market_Cluster_ID'] = 'CarolEstates'\n",
    "result.loc[result['prop_id'].isin(Westchesterish['prop_id']), 'Market_Cluster_ID'] = 'Westchesterish'\n",
    "result.loc[result['prop_id'].isin(QuailCreekish['prop_id']), 'Market_Cluster_ID'] = 'QuailCreekish'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market Clusters Map\n",
    "Please work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    iframe {\n",
       "        margin-left: auto !important;\n",
       "        margin-right: auto !important;\n",
       "        display: block;\n",
       "        border: 2px solid #ddd;\n",
       "        border-radius: 8px;\n",
       "    }\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"80%\"\n",
       "            height=\"600px\"\n",
       "            src=\"market_clusters_map.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x251c4e3a6c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from IPython.display import IFrame, display, HTML\n",
    "\n",
    "# Use the provided MapData as IntMapData\n",
    "IntMapData = result\n",
    "\n",
    "# Calculate the mean latitude and longitude\n",
    "center_lat = IntMapData['CENTROID_Y'].mean()\n",
    "center_lon = IntMapData['CENTROID_X'].mean()\n",
    "\n",
    "# Create the map centered on the mean location\n",
    "map_clusters = folium.Map(\n",
    "    location=[center_lat, center_lon],\n",
    "    zoom_start=6,\n",
    "    tiles='OpenStreetMap',\n",
    "    width='80%',\n",
    "    height='600px'\n",
    ")\n",
    "\n",
    "# Generate 40 unique colors using the updated Matplotlib colormaps API\n",
    "cmap = plt.colormaps[\"tab20b\"]  # Tab20b colormap for distinct colors\n",
    "color_list = [mcolors.rgb2hex(cmap(i / 20)) for i in range(20)] * 2  # Repeat to ensure 40 colors\n",
    "\n",
    "# Assign unique colors to each Market_Cluster_ID\n",
    "unique_clusters = IntMapData['Market_Cluster_ID'].unique()\n",
    "cluster_colors = {\n",
    "    cluster_id: color for cluster_id, color in zip(\n",
    "        unique_clusters, color_list[:len(unique_clusters)]\n",
    "    )\n",
    "}\n",
    "\n",
    "# Automatically calculate map bounds\n",
    "min_lat, max_lat = IntMapData['CENTROID_Y'].min(), IntMapData['CENTROID_Y'].max()\n",
    "min_lon, max_lon = IntMapData['CENTROID_X'].min(), IntMapData['CENTROID_X'].max()\n",
    "\n",
    "# Fit map to bounds dynamically based on displayed points\n",
    "def update_bounds(points):\n",
    "    latitudes = points['CENTROID_Y']\n",
    "    longitudes = points['CENTROID_X']\n",
    "    return [[latitudes.min(), longitudes.min()], [latitudes.max(), longitudes.max()]]\n",
    "\n",
    "bounds = update_bounds(IntMapData)\n",
    "map_clusters.fit_bounds(bounds)\n",
    "\n",
    "# Create feature groups for each market cluster\n",
    "for cluster_id in unique_clusters:\n",
    "    cluster_group = folium.FeatureGroup(name=f\"Cluster {cluster_id}\", show=False)  # Default to off\n",
    "\n",
    "    # Add properties to the respective cluster\n",
    "    cluster_data = IntMapData[IntMapData['Market_Cluster_ID'] == cluster_id]\n",
    "    for _, row in cluster_data.iterrows():\n",
    "        folium.CircleMarker(\n",
    "            location=[row['CENTROID_Y'], row['CENTROID_X']],\n",
    "            radius=3,  # Smaller size for better visualization\n",
    "            color=cluster_colors.get(cluster_id, 'gray'),\n",
    "            fill=True,\n",
    "            fill_color=cluster_colors.get(cluster_id, 'gray'),\n",
    "            fill_opacity=0.8,\n",
    "            popup=f\"<strong>Market Area:</strong> {row['Market_Cluster_ID']}\"\n",
    "        ).add_to(cluster_group)\n",
    "\n",
    "    # Add the cluster group to the map\n",
    "    map_clusters.add_child(cluster_group)\n",
    "\n",
    "# Add a layer control to toggle groups\n",
    "folium.LayerControl(collapsed=False).add_to(map_clusters)\n",
    "\n",
    "# Inject JavaScript for dynamic autozoom functionality\n",
    "map_clusters.get_root().html.add_child(folium.Element(\"\"\"\n",
    "<script>\n",
    "document.addEventListener(\"DOMContentLoaded\", function() {\n",
    "    var mapElement = document.querySelector('div[id^=\"map_\"]'); // Find the map element\n",
    "    if (mapElement) {\n",
    "        var mapId = mapElement.id; // Get the map's unique ID\n",
    "        var mapInstance = window[mapId]; // Access the map object\n",
    "\n",
    "        function updateMapBounds() {\n",
    "            var bounds = new L.LatLngBounds();\n",
    "            mapInstance.eachLayer(function (layer) {\n",
    "                if (layer instanceof L.LayerGroup && mapInstance.hasLayer(layer)) {\n",
    "                    layer.eachLayer(function (subLayer) {\n",
    "                        if (subLayer.getLatLng) {\n",
    "                            bounds.extend(subLayer.getLatLng());\n",
    "                        }\n",
    "                    });\n",
    "                }\n",
    "            });\n",
    "            if (!bounds.isValid()) {\n",
    "                return;\n",
    "            }\n",
    "            mapInstance.fitBounds(bounds);\n",
    "        }\n",
    "\n",
    "        // Attach the updateMapBounds function to layer events\n",
    "        mapInstance.on('overlayadd', updateMapBounds);\n",
    "        mapInstance.on('overlayremove', updateMapBounds);\n",
    "    }\n",
    "});\n",
    "</script>\n",
    "\"\"\"))\n",
    "\n",
    "# Save the map to an HTML file\n",
    "map_file = \"market_clusters_map.html\"\n",
    "map_clusters.save(map_file)\n",
    "\n",
    "# Apply CSS for centering the iframe\n",
    "centered_css = \"\"\"<style>\n",
    "    iframe {\n",
    "        margin-left: auto !important;\n",
    "        margin-right: auto !important;\n",
    "        display: block;\n",
    "        border: 2px solid #ddd;\n",
    "        border-radius: 8px;\n",
    "    }\n",
    "</style>\"\"\"\n",
    "\n",
    "display(HTML(centered_css))\n",
    "\n",
    "# Embed the saved map into the notebook\n",
    "display(IFrame(src=map_file, width='80%', height='600px'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dummy variables for non-numeric data\n",
    "Since several of the factors are not numbers we need to make binary dummy variables so the regression can recognize their impact. Dummy variables put different values for a given factor into categories and each property is either 1 or 0 (true or false) for each category. Some of the market clusters also have to have their names slightly tweaked so that python will behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dummy variables...\n",
      "Renaming columns with problematic characters...\n"
     ]
    }
   ],
   "source": [
    "# Create dummy variables for non-numeric data\n",
    "print(\"Creating dummy variables...\")\n",
    "# Join dummy variables for 'tax_area_description' and 'Market_Cluster_ID'\n",
    "result = result.join(pd.get_dummies(result.tax_area_description))\n",
    "result = result.join(pd.get_dummies(result.Market_Cluster_ID))\n",
    "\n",
    "# Rename columns that will act up in Python\n",
    "print(\"Renaming columns with problematic characters...\")\n",
    "# Rename columns to avoid issues with special characters or spaces\n",
    "column_mapping = {\n",
    "    'HIGH SPRINGS': 'HIGH_SPRINGS',\n",
    "    \"ST. JOHN'S\": 'ST_JOHNS'\n",
    "}\n",
    "result.rename(columns=column_mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large acerage exclusion\n",
    "The maximum legal acreage for the model is set to `legalAcreageMax`, which is currently 10 acres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variable\n",
    "legalAcreageMax = 10  # in acres\n",
    "\n",
    "result = result[result['legal_acreage'] <= legalAcreageMax]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensuring all column names are strings\n",
    "Data type mismatches cause annoying errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that all column names are strings\n",
    "result.columns = result.columns.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "### Formula\n",
    "I decided to go with a log based regression formula because most of the numeric factors we use are not normally distributed. Here it is in markdown for the sake of readability:\n",
    "\n",
    "regressionFormula = \"np.log(Assessment_Val) ~ np.log(living_area) + np.log(landiness) + np.log(percent_good) + np.log(imprv_det_quality_cd) + np.log(total_porch_area + 1) + np.log(total_garage_area + 1) + Springtree_B + HighSprings_A + MidtownEast_C + swNewberry_B + MidtownEast_A + swNewberry_A + MidtownEast_B + HighSprings_F + Springtree_A + Tioga_B + Tioga_A + MidtownEast_D + WaldoRural_A + Alachua_Main + High_Springs_Main + HaileLike + HighSprings_B + Real_Tioga + Duck_Pond + Newmans_Lake + EastMidtownEastA + HighSpringsAGNV + Hawthorne + HighSprings_B + Golfview + Lugano + Archer + WildsPlantation+Buck_Bay+in_subdivision+has_lake+WaldoRural_C+HighSprings_E+HSBUI+number_of_baths+EastGNV+Ironwood+SummerCreek+has_canal+TC_Forest+CarolEstates+Westchesterish+QuailCreekish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressionFormula = \"np.log(Assessment_Val) ~ np.log(living_area) + np.log(landiness) + np.log(percent_good) + np.log(imprv_det_quality_cd) + np.log(total_porch_area + 1) + np.log(total_garage_area + 1) + Springtree_B + HighSprings_A + MidtownEast_C + swNewberry_B + MidtownEast_A + swNewberry_A + MidtownEast_B + HighSprings_F + Springtree_A + Tioga_B + Tioga_A + MidtownEast_D + WaldoRural_A + Alachua_Main + High_Springs_Main + HaileLike + HighSprings_B + Real_Tioga + Duck_Pond + Newmans_Lake + EastMidtownEastA + HighSpringsAGNV + Hawthorne + HighSprings_B + Golfview + Lugano + Archer + WildsPlantation+Buck_Bay+in_subdivision+has_lake+WaldoRural_C+HighSprings_E+HSBUI+number_of_baths+EastGNV+Ironwood+SummerCreek+has_canal+TC_Forest+CarolEstates+Westchesterish+QuailCreekish\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "The data is split into training and testing sets. The training data is used to inform the model, the test data is used to check the performance of the trained model. The split takes out a random 20% of the properties to use for testing but for my purposes I've been using the same random seed so that variation in the results is from changes I make to the model and not from just getting a different seed. I believe the plan in the future will be to run the model on multiple seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into training and test sets...\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "print(\"Splitting data into training and test sets...\")\n",
    "test_size_var = 0.2\n",
    "train_data, test_data = train_test_split(result, test_size=test_size_var, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression run\n",
    "This is where the regression is actually run and the statistical summary generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the regression model...\n",
      "Regression model summary:\n",
      "                              OLS Regression Results                              \n",
      "==================================================================================\n",
      "Dep. Variable:     np.log(Assessment_Val)   R-squared:                       0.927\n",
      "Model:                                OLS   Adj. R-squared:                  0.926\n",
      "Method:                     Least Squares   F-statistic:                     1440.\n",
      "Date:                    Mon, 16 Dec 2024   Prob (F-statistic):               0.00\n",
      "Time:                            14:57:31   Log-Likelihood:                 4048.7\n",
      "No. Observations:                    5523   AIC:                            -7999.\n",
      "Df Residuals:                        5474   BIC:                            -7675.\n",
      "Df Model:                              48                                         \n",
      "Covariance Type:                nonrobust                                         \n",
      "=================================================================================================\n",
      "                                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Intercept                         8.4298      0.058    144.602      0.000       8.316       8.544\n",
      "Springtree_B[T.True]              0.0032      0.007      0.479      0.632      -0.010       0.016\n",
      "HighSprings_A[T.True]            -0.0068      0.010     -0.665      0.506      -0.027       0.013\n",
      "MidtownEast_C[T.True]             0.0019      0.010      0.197      0.844      -0.017       0.021\n",
      "swNewberry_B[T.True]              0.0552      0.010      5.759      0.000       0.036       0.074\n",
      "MidtownEast_A[T.True]             0.1394      0.013     11.046      0.000       0.115       0.164\n",
      "swNewberry_A[T.True]             -0.1128      0.009    -12.759      0.000      -0.130      -0.095\n",
      "MidtownEast_B[T.True]             0.0747      0.009      7.950      0.000       0.056       0.093\n",
      "HighSprings_F[T.True]             0.1006      0.012      8.335      0.000       0.077       0.124\n",
      "Springtree_A[T.True]             -0.0284      0.018     -1.544      0.123      -0.064       0.008\n",
      "Tioga_B[T.True]                   0.1242      0.016      7.729      0.000       0.093       0.156\n",
      "Tioga_A[T.True]                   0.1264      0.018      7.079      0.000       0.091       0.161\n",
      "MidtownEast_D[T.True]            -0.0888      0.019     -4.600      0.000      -0.127      -0.051\n",
      "WaldoRural_A[T.True]             -0.5065      0.029    -17.477      0.000      -0.563      -0.450\n",
      "Alachua_Main[T.True]             -0.0922      0.010     -9.649      0.000      -0.111      -0.073\n",
      "High_Springs_Main[T.True]        -0.0974      0.009    -11.059      0.000      -0.115      -0.080\n",
      "HaileLike[T.True]                 0.1422      0.008     17.232      0.000       0.126       0.158\n",
      "HighSprings_B[T.True]            -0.0042      0.006     -0.668      0.504      -0.017       0.008\n",
      "Real_Tioga[T.True]                0.2516      0.014     18.048      0.000       0.224       0.279\n",
      "Duck_Pond[T.True]                 0.2930      0.017     17.708      0.000       0.261       0.325\n",
      "Newmans_Lake[T.True]             -0.2555      0.012    -20.754      0.000      -0.280      -0.231\n",
      "EastMidtownEastA[T.True]          0.2939      0.012     23.554      0.000       0.269       0.318\n",
      "HighSpringsAGNV[T.True]           0.1053      0.008     12.747      0.000       0.089       0.121\n",
      "Hawthorne[T.True]                -0.3286      0.020    -16.290      0.000      -0.368      -0.289\n",
      "Golfview[T.True]                  0.4542      0.033     13.850      0.000       0.390       0.518\n",
      "Lugano[T.True]                    0.1578      0.048      3.294      0.001       0.064       0.252\n",
      "Archer[T.True]                   -0.2122      0.024     -8.975      0.000      -0.259      -0.166\n",
      "WildsPlantation[T.True]           0.2567      0.020     12.842      0.000       0.218       0.296\n",
      "Buck_Bay[T.True]                 -0.1365      0.027     -5.128      0.000      -0.189      -0.084\n",
      "in_subdivision[T.True]            0.0249      0.007      3.331      0.001       0.010       0.040\n",
      "WaldoRural_C[T.True]             -0.1877      0.016    -11.645      0.000      -0.219      -0.156\n",
      "HighSprings_E[T.True]            -0.2239      0.039     -5.685      0.000      -0.301      -0.147\n",
      "HSBUI[T.True]                    -0.0756      0.015     -5.007      0.000      -0.105      -0.046\n",
      "EastGNV[T.True]                  -0.3087      0.012    -25.081      0.000      -0.333      -0.285\n",
      "Ironwood[T.True]                 -0.1422      0.037     -3.820      0.000      -0.215      -0.069\n",
      "SummerCreek[T.True]               0.1025      0.022      4.649      0.000       0.059       0.146\n",
      "TC_Forest[T.True]                -0.0510      0.018     -2.791      0.005      -0.087      -0.015\n",
      "CarolEstates[T.True]             -0.1685      0.023     -7.257      0.000      -0.214      -0.123\n",
      "Westchesterish[T.True]           -0.0996      0.022     -4.595      0.000      -0.142      -0.057\n",
      "QuailCreekish[T.True]            -0.0152      0.012     -1.242      0.214      -0.039       0.009\n",
      "np.log(living_area)               0.5581      0.008     66.332      0.000       0.542       0.575\n",
      "np.log(landiness)                 0.0800      0.002     34.371      0.000       0.075       0.085\n",
      "np.log(percent_good)              0.7491      0.013     58.131      0.000       0.724       0.774\n",
      "np.log(imprv_det_quality_cd)      0.7306      0.026     28.551      0.000       0.680       0.781\n",
      "np.log(total_porch_area + 1)      0.0053      0.001      5.158      0.000       0.003       0.007\n",
      "np.log(total_garage_area + 1)     0.0149      0.001     19.596      0.000       0.013       0.016\n",
      "has_lake                          0.3237      0.027     12.033      0.000       0.271       0.376\n",
      "number_of_baths                   0.0371      0.004     10.146      0.000       0.030       0.044\n",
      "has_canal                         0.1223      0.047      2.620      0.009       0.031       0.214\n",
      "==============================================================================\n",
      "Omnibus:                      191.454   Durbin-Watson:                   1.971\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              505.153\n",
      "Skew:                           0.118   Prob(JB):                    2.03e-110\n",
      "Kurtosis:                       4.463   Cond. No.                         392.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Fit the regression model\n",
    "print(\"Fitting the regression model...\")\n",
    "regresult = smf.ols(formula=regressionFormula, data=train_data).fit()\n",
    "# Display regression summary\n",
    "print(\"Regression model summary:\")\n",
    "print(regresult.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model performance with appraisal metrics\n",
    "The model generated by the training data and regression is used to predict the assessment value of the test data properties and then compared to the actual sale price of those properties in order to evaluate performance. The goal is to achieve a sale ratio of 0.85 in line with local laws and appraisal standards.\n",
    "\n",
    "Metrics evaluated:\n",
    "\n",
    "- Mean absolute error: Measures the average absolute difference between the predicted total (Assessment Value + MISC Value) and the actual sale price.\n",
    "\n",
    "- Mean absolute error 2: Measures the average absolute difference between the predicted assessment values and the actual assessment values (calculated as Sale Price - MISC Value). The target value for both measures of MAE is as close to zero as possible. \n",
    "\n",
    "- Price Related Differential: Measures assessment equity; calculated as the mean assessment ratio divided by the weighted mean assessment ratio. Per IAAO standards a PRD value between 0.98 and 1.03 is considered acceptable. \n",
    "\n",
    "- Coefficient of Dispersion: Measures the average absolute percentage deviation of the ratios from the median ratio. Per IAAO standards a COD value between 5.0 and 10.0 is considered acceptable.  \n",
    "\n",
    "- Price related bias: The PRB provides a percentage by which\n",
    "model-derived value estimates rise or fall as values double.Per IAAO standards, the PRB coefficient should fall between â€“0.05 and 0.05.\n",
    "\n",
    "- Weighted Mean: Averages the ratios of assessment values to sale prices, weighting by the sale price. Used to evaluate overall equity in assessments. The aim is to be as close to 0.85 as possible.\n",
    "\n",
    "- Mean sales ratio: The unweighted average of assessment-to-sale price ratios across all properties, providing a simple measure of assessment equity. The aim is to be as close to 0.85 as possible.\n",
    "\n",
    "- Median sales ratio: The middle value of the assessment-to-sale price ratios when sorted in order. Often preferred for its resistance to outliers and skewed distributions. The aim is to be as close to 0.85 as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model performance on test data...\n",
      "Calculating performance metrics...\n",
      "Mean Absolute Error: 62297.31488239585\n",
      "Mean Absolute Error_2: 27945.60296816608\n",
      "PRD: 1.0126748104369767\n",
      "COD: 8.720269913565843\n",
      "PRB: {'PRB': -0.0445498393613329, 'Sig': 5.159762707483695e-20}\n",
      "weightedMean: 0.8427682502766741\n",
      "meanRatio: 0.8534501780912335\n",
      "medianRatio: 0.8483357350395878\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model performance on test data...\")\n",
    "# Get predictions to test\n",
    "predictions = test_data.copy()\n",
    "# Predict log-transformed assessment values\n",
    "predictions['predicted_log_Assessment_Val'] = regresult.predict(predictions)\n",
    "# Convert predicted log values to original scale\n",
    "predictions['predicted_Assessment_Val'] = np.exp(predictions['predicted_log_Assessment_Val'])\n",
    "# Define actual and predicted values for further evaluation\n",
    "actual_values = predictions['sl_price']\n",
    "predicted_values = predictions['predicted_Assessment_Val'] + predictions['MISC_Val']\n",
    "predicted_values_mae = predictions['predicted_Assessment_Val']\n",
    "actual_values_mae = predictions['Assessment_Val']\n",
    "\n",
    "# Test predictions on performance metrics\n",
    "print(\"Calculating performance metrics...\")\n",
    "mae = mean_absolute_error(predicted_values, actual_values)\n",
    "mae_2 = mean_absolute_error(predicted_values_mae, actual_values_mae)\n",
    "# Calculate IAAO metrics\n",
    "PRD_table = PRD(predicted_values, actual_values)\n",
    "COD_table = COD(predicted_values, actual_values)\n",
    "PRB_table = PRB(predicted_values, actual_values)\n",
    "wm = weightedMean(predicted_values, actual_values)\n",
    "meanRatio = (predicted_values / actual_values).mean()\n",
    "medianRatio = (predicted_values / actual_values).median()\n",
    "\n",
    "# Print performance metrics\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"Mean Absolute Error_2: {mae_2}\")\n",
    "print(f\"PRD: {PRD_table}\")\n",
    "print(f\"COD: {COD_table}\")\n",
    "print(f\"PRB: {PRB_table}\")\n",
    "print(f\"weightedMean: {wm}\")\n",
    "print(f\"meanRatio: {meanRatio}\")\n",
    "print(f\"medianRatio: {medianRatio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's next?\n",
    "- The model struggles with extremely large homes (mansions) in rural areas.\n",
    "- Quality Code lineraization could probably be better\n",
    "- The market clusters still need some tweaking but I'm a little unsure where to go next with them.\n",
    "- Need subject matter experts to review some of the other 1.5 * IQR outliers\n",
    "- Need a way to assign market clusters to properties that aren't in the sale data for when we eventually deploy this. Working on a mix of random forest predition model with some kind of spatial component and then some neighborhoods we could probably just cut by hand.\n",
    "- Whatever Michael tells me to do"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
